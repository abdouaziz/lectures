{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer_Encoder.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_NCwdDbLij_"
      },
      "source": [
        "import tensorflow as tf \n",
        "import numpy as np "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1G28oXqMQK9"
      },
      "source": [
        "input_embedding = [[\n",
        "                \"salut\" , \"comment\",\"ca\",\"va\" , \"?\"\n",
        "]]\n",
        "\n",
        "output_embedding = [[\n",
        "                   \"<START>\" ,  \"hi\",\"how\",\"are\",\"you\" , \"?\"\n",
        "]]\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zve5Eg4MYJ6"
      },
      "source": [
        "def get_vocab(sentences):\n",
        "  vocab = {}\n",
        "  for sentence in sentences:\n",
        "    for word in sentence :\n",
        "      if word not in vocab:\n",
        "        vocab[word] = len(vocab)\n",
        "\n",
        "  return vocab"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3PW3Fw2Mzoa"
      },
      "source": [
        "input_vocab = get_vocab(input_embedding)\n",
        "output_vocab =  get_vocab(output_embedding)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qx_YbBOgP2Gp"
      },
      "source": [
        "input_vocab[\"<START>\"] =  len(input_vocab)\n",
        "input_vocab[\"<END>\"] =  len(input_vocab)\n",
        "input_vocab[\"<PAD>\"] =  len(input_vocab)\n",
        "\n",
        "\n",
        "output_vocab[\"<START>\"] =  len(input_vocab)\n",
        "output_vocab[\"<END>\"] =  len(input_vocab)\n",
        "output_vocab[\"<PAD>\"] =  len(input_vocab)\n",
        "\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWshe4mdM0kl",
        "outputId": "d7e9418d-0590-4695-ee39-e1f7a2c70d0d"
      },
      "source": [
        "input_vocab"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<END>': 6,\n",
              " '<PAD>': 7,\n",
              " '<START>': 5,\n",
              " '?': 4,\n",
              " 'ca': 2,\n",
              " 'comment': 1,\n",
              " 'salut': 0,\n",
              " 'va': 3}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fvUnOTGNhkr",
        "outputId": "9d8e67b4-363d-4cc9-954d-2f5d9868a578"
      },
      "source": [
        "output_vocab"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<END>': 8,\n",
              " '<PAD>': 8,\n",
              " '<START>': 8,\n",
              " '?': 5,\n",
              " 'are': 3,\n",
              " 'hi': 1,\n",
              " 'how': 2,\n",
              " 'you': 4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALjAcG9QPmy7"
      },
      "source": [
        "def sequence_to_str(sequences, vocab):\n",
        "  for sequence in sequences:\n",
        "    for s,word in enumerate(sequence):\n",
        "      sequence[s] = vocab[word]\n",
        "  return np.array(sequences)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6iNEJq9TBMX"
      },
      "source": [
        "input_sequences =  sequence_to_str(input_embedding , input_vocab)\n",
        "output_sequences  = sequence_to_str(output_embedding , output_vocab)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ju92qe8OTFKS",
        "outputId": "cb58a335-c4b1-4eee-f359-7670c8493033"
      },
      "source": [
        "output_sequences"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[8, 1, 2, 3, 4, 5]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5zTu8hhFRJF"
      },
      "source": [
        "class Embedd(tf.keras.layers.Layer):\n",
        "  def __init__(self,input_dim,output_dim,):\n",
        "    super(Embedd , self).__init__() \n",
        "    self.emb = tf.keras.layers.Embedding(input_dim , output_dim)\n",
        "  def call(self ,x):\n",
        "    x =  self.emb(x)\n",
        "  \n",
        "    return x "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YF8m6myPTSZL"
      },
      "source": [
        "class ScaleDotProduct(tf.keras.layers.Layer):\n",
        "  def __init__(self , units):\n",
        "    super(ScaleDotProduct , self).__init__()\n",
        "    self.q_layers =  tf.keras.layers.Dense(units)\n",
        "    self.k_layers =  tf.keras.layers.Dense(units)\n",
        "    self.v_layers =  tf.keras.layers.Dense(units)\n",
        "  \n",
        "  def call(self ,x):\n",
        "    Q =  self.q_layers(x)\n",
        "    K =  self.k_layers(x)\n",
        "    V = self.v_layers(x)\n",
        "  \n",
        "    QK =  tf.matmul(Q,K , transpose_b=True)\n",
        "    QK = QK / tf.math.sqrt(256.)\n",
        "    softmax_QK = tf.nn.softmax(QK, axis=-1)\n",
        "    attention = tf.matmul(softmax_QK, V)\n",
        "    \n",
        "    return attention "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zo5FZRmkHTh6"
      },
      "source": [
        "#Part for the Multiple Head of Attention "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-cs8IBv-T0Q"
      },
      "source": [
        "#Multi head Attention\n",
        "\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self,units ,dim=256 ,nb_head=8):\n",
        "    super(MultiHeadAttention , self).__init__() \n",
        "    self.head_dim = dim//nb_head\n",
        "    self.dim =  dim\n",
        "    self.nb_head =  nb_head\n",
        "    self.q_layers =  tf.keras.layers.Dense(units)\n",
        "    self.k_layers =  tf.keras.layers.Dense(units)\n",
        "    self.v_layers =  tf.keras.layers.Dense(units)\n",
        "    self.out_proj = tf.keras.layers.Dense(units)\n",
        "  \n",
        "  def call(self ,x):\n",
        "    Q =  self.q_layers(x)\n",
        "    K =  self.k_layers(x)\n",
        "    V = self.v_layers(x)\n",
        "\n",
        "\n",
        " \n",
        "    batch_size = tf.shape(Q)[0]\n",
        "    seq_len = tf.shape(Q)[1] \n",
        "\n",
        "    Q = tf.reshape(Q , [batch_size , seq_len , self.nb_head , self.head_dim])\n",
        "    K = tf.reshape(K , [batch_size , seq_len , self.nb_head , self.head_dim])\n",
        "    V = tf.reshape(V , [batch_size , seq_len , self.nb_head , self.head_dim])\n",
        "\n",
        "  \n",
        "\n",
        "    Q =  tf.transpose(Q , [0 , 2,1,3])\n",
        "    K =  tf.transpose(K , [0 , 2,1,3])\n",
        "    V =  tf.transpose(V , [0 , 2,1,3])\n",
        "    \n",
        "  \n",
        "    Q = tf.reshape(Q , [batch_size*self.nb_head , seq_len , self.head_dim])\n",
        "    K = tf.reshape(K , [batch_size*self.nb_head , seq_len , self.head_dim])\n",
        "    V = tf.reshape(V , [batch_size*self.nb_head , seq_len , self.head_dim])\n",
        " \n",
        "\n",
        "    QK =  tf.matmul(Q,K , transpose_b=True)\n",
        "    QK = QK / tf.math.sqrt(256.)\n",
        "    softmax_QK = tf.nn.softmax(QK, axis=-1)\n",
        "    attention = tf.matmul(softmax_QK, V)\n",
        "    attention = tf.reshape(attention , [batch_size , self.nb_head , seq_len , self.head_dim])\n",
        "\n",
        "    attention = tf.transpose(attention , [0,2,1,3])\n",
        "    #The Concat \n",
        "    attention = tf.reshape(attention , [batch_size  , seq_len , self.nb_head*self.head_dim])\n",
        "\n",
        "    out_attention = self.out_proj(attention)\n",
        "\n",
        "\n",
        "    return out_attention"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ku-BGRFEsSG6"
      },
      "source": [
        "class EncodeLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,units):\n",
        "    super(EncodeLayer , self).__init__()\n",
        "    self.mult_head_attention =  MultiHeadAttention(units)\n",
        "    self.norm = tf.keras.layers.BatchNormalization()\n",
        "    self.dense = tf.keras.layers.Dense(units)\n",
        "    \n",
        "  def call(self ,x):\n",
        "    attention=  self.mult_head_attention(x)\n",
        "    post_attention = self.norm(x+attention)\n",
        "    x =  self.dense(post_attention)\n",
        "    enc_output = self.norm(x +post_attention)\n",
        "  \n",
        "    return enc_output"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZsVWeLtWmp8"
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self , units , nb_encoder):\n",
        "    super(Encoder , self).__init__()\n",
        "    self.nb_encoder =  nb_encoder\n",
        "    self.units = units\n",
        "  \n",
        "  def build(self , input_shape):\n",
        "    super().build(input_shape)\n",
        "\n",
        "    self.encoder_layers =[]\n",
        "\n",
        "    for nb in range(self.nb_encoder):\n",
        "      self.encoder_layers.append(EncodeLayer(self.units))\n",
        " \n",
        "  def call(self ,x):\n",
        "\n",
        "    for encoder_layer in self.encoder_layers:\n",
        "      x = encoder_layer(x)\n",
        "       \n",
        "    return x"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eisXlVXlWfc"
      },
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self,units , input_dim,output_dim , nb_encoder):\n",
        "    super(MyModel ,self ).__init__()\n",
        "    self.emb = Embedd(input_dim , output_dim)\n",
        "    self.encoder = Encoder(units , nb_encoder)\n",
        "    \n",
        "  def call(self,x):\n",
        "    embedd =  self.emb(x)\n",
        "    encoder_output = self.encoder(embedd)\n",
        "\n",
        "    return encoder_output \n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgZuovB0lxYs"
      },
      "source": [
        "model = MyModel(256,5,256,6)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWQ3g5JIl4OZ"
      },
      "source": [
        "y =model(input_sequences)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDgqltVV7p_c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e863dbfe-f1ad-4d54-9867-b4391cd48f5d"
      },
      "source": [
        "y"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 5, 256), dtype=float32, numpy=\n",
              "array([[[-0.45936322, -1.5659549 , -0.48583964, ...,  1.3093379 ,\n",
              "          0.3229911 ,  0.8611849 ],\n",
              "        [-0.30107084, -1.8425404 , -1.0650803 , ...,  0.9380076 ,\n",
              "          0.64954585,  0.71157306],\n",
              "        [-0.16058953, -1.5436242 , -0.965643  , ...,  1.154702  ,\n",
              "          0.49532196,  0.68222964],\n",
              "        [-0.4332395 , -1.6017998 , -0.6330609 , ...,  1.141011  ,\n",
              "          0.4485056 ,  1.0755699 ],\n",
              "        [-0.5741957 , -1.2223752 , -1.0310805 , ...,  1.1801189 ,\n",
              "          1.0313195 ,  0.8484405 ]]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXRT0JjQzCdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cf5f546-6fcc-46c3-fdc1-2ad03460086a"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"my_model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedd_2 (Embedd)            multiple                  1280      \n",
            "_________________________________________________________________\n",
            "encoder (Encoder)            multiple                  1979904   \n",
            "=================================================================\n",
            "Total params: 1,981,184\n",
            "Trainable params: 1,978,112\n",
            "Non-trainable params: 3,072\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndvNBDrPmAJV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}